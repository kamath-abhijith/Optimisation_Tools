{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## FUNCTION DEFINITIONS\n",
    "\n",
    "def grad_p1(x):\n",
    "    C = np.array([[np.sqrt(2),0,0],\n",
    "                  [0,1,0],\n",
    "                  [0,0,1]])\n",
    "    d = np.array([2,3,1])\n",
    "    return C.T.dot(C.dot(x)+d)\n",
    "\n",
    "def proj_p1(x):\n",
    "    A = np.array([[1,-1,0],\n",
    "                  [0,1,-1]])\n",
    "    b = np.array([2,3])\n",
    "    return x - A.T.dot(np.linalg.solve(A.dot(A.T),A.dot(x)-b))\n",
    "\n",
    "def grad_descent_p1(x_init, tol=1e-6, max_iterations=100):\n",
    "\n",
    "    xsol = np.array([0.0429,-1.9571,-4.9571])\n",
    "    x = x_init\n",
    "    for iter in range(max_iterations):\n",
    "        grad_x = grad_p1(x)\n",
    "        # step_size = backtracking(x, f, grad, rho, beta)\n",
    "        step_size = 0.5\n",
    "\n",
    "        x -= step_size*grad_x\n",
    "        x = proj_p1(x)\n",
    "        if  np.linalg.norm(x-xsol) <= tol:\n",
    "            break\n",
    "\n",
    "    return iter+1, x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Gradient descent with exact step size takes 8 iterations to solve for the minimiser with an error of 0.0010000\n"
     ]
    }
   ],
   "source": [
    "tol = 1e-3\n",
    "max_iterations = 10000\n",
    "x_init = np.array([0.0,0.0,0.0], dtype=np.float64)\n",
    "\n",
    "num_iter, x_gd = grad_descent_p1(x_init, tol, max_iterations)\n",
    "\n",
    "print(\"Gradient descent with exact step size takes %d iterations to solve for the minimiser with an error of %.7f\"%(num_iter, tol))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## FUNCTION DEFINITIONS\n",
    "\n",
    "def grad_p2(x):\n",
    "    C = np.array([[1,2],\n",
    "                  [2,1],\n",
    "                  [1,1]])\n",
    "    d = np.array([3,4,5])\n",
    "    return 2*C.T.dot(C.dot(x)+d)\n",
    "\n",
    "def proj_C(x):\n",
    "    return x/max(1,np.linalg.norm(x))\n",
    "\n",
    "def proj_D(x):\n",
    "    a = np.array([-1,1])\n",
    "    r = 1/np.sqrt(2)\n",
    "\n",
    "    if np.linalg.norm(x-a)<=r:\n",
    "        return x\n",
    "    \n",
    "    else:\n",
    "        return r*(x-a)/np.linalg.norm(x-a) + a\n",
    "\n",
    "def proj_CD(x):\n",
    "    w = proj_C(x)\n",
    "    for i in range(100):\n",
    "        y = proj_D(w)\n",
    "        w = proj_C(y)\n",
    "\n",
    "        if np.linalg.norm(w-y) <= 0.0001:\n",
    "            break\n",
    "    \n",
    "    return w\n",
    "\n",
    "def grad_descent_p2(x_init, tol=1e-6, max_iterations=100):\n",
    "\n",
    "    # xsol = np.array([0.0429,-1.9571,-4.9571])\n",
    "    x = x_init\n",
    "    for iter in range(max_iterations):\n",
    "        grad_x = grad_p2(x)\n",
    "        # step_size = backtracking(x, f, grad, rho, beta)\n",
    "        step_size = 0.01\n",
    "\n",
    "        x -= step_size*grad_x\n",
    "        x = proj_CD(x)\n",
    "        print(x)\n",
    "        # if  np.linalg.norm(x-xsol) <= tol:\n",
    "        # if np.linalg.norm(grad_x) <= tol:\n",
    "            # break\n",
    "\n",
    "    return iter+1, x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[-0.40315649  0.62081953]\n[-0.75528548  0.33658851]\n[-0.95572614  0.29425763]\n[-0.95572629  0.29425713]\n[-0.95572629  0.29425713]\n[-0.95572629  0.29425713]\n[-0.95572629  0.29425713]\n[-0.95572629  0.29425713]\n[-0.95572629  0.29425713]\n[-0.95572629  0.29425713]\n[-0.95572629  0.29425713]\n[-0.95572629  0.29425713]\n[-0.95572629  0.29425713]\n[-0.95572629  0.29425713]\n[-0.95572629  0.29425713]\n[-0.95572629  0.29425713]\n[-0.95572629  0.29425713]\n[-0.95572629  0.29425713]\n[-0.95572629  0.29425713]\n[-0.95572629  0.29425713]\n[-0.95572629  0.29425713]\n[-0.95572629  0.29425713]\n[-0.95572629  0.29425713]\n[-0.95572629  0.29425713]\n[-0.95572629  0.29425713]\n[-0.95572629  0.29425713]\n[-0.95572629  0.29425713]\n[-0.95572629  0.29425713]\n[-0.95572629  0.29425713]\n[-0.95572629  0.29425713]\n[-0.95572629  0.29425713]\n[-0.95572629  0.29425713]\n[-0.95572629  0.29425713]\n[-0.95572629  0.29425713]\n[-0.95572629  0.29425713]\n[-0.95572629  0.29425713]\n[-0.95572629  0.29425713]\n[-0.95572629  0.29425713]\n[-0.95572629  0.29425713]\n[-0.95572629  0.29425713]\n[-0.95572629  0.29425713]\n[-0.95572629  0.29425713]\n[-0.95572629  0.29425713]\n[-0.95572629  0.29425713]\n[-0.95572629  0.29425713]\n[-0.95572629  0.29425713]\n[-0.95572629  0.29425713]\n[-0.95572629  0.29425713]\n[-0.95572629  0.29425713]\n[-0.95572629  0.29425713]\n[-0.95572629  0.29425713]\n[-0.95572629  0.29425713]\n[-0.95572629  0.29425713]\n[-0.95572629  0.29425713]\n[-0.95572629  0.29425713]\n[-0.95572629  0.29425713]\n[-0.95572629  0.29425713]\n[-0.95572629  0.29425713]\n[-0.95572629  0.29425713]\n[-0.95572629  0.29425713]\n[-0.95572629  0.29425713]\n[-0.95572629  0.29425713]\n[-0.95572629  0.29425713]\n[-0.95572629  0.29425713]\n[-0.95572629  0.29425713]\n[-0.95572629  0.29425713]\n[-0.95572629  0.29425713]\n[-0.95572629  0.29425713]\n[-0.95572629  0.29425713]\n[-0.95572629  0.29425713]\n[-0.95572629  0.29425713]\n[-0.95572629  0.29425713]\n[-0.95572629  0.29425713]\n[-0.95572629  0.29425713]\n[-0.95572629  0.29425713]\n[-0.95572629  0.29425713]\n[-0.95572629  0.29425713]\n[-0.95572629  0.29425713]\n[-0.95572629  0.29425713]\n[-0.95572629  0.29425713]\n[-0.95572629  0.29425713]\n[-0.95572629  0.29425713]\n[-0.95572629  0.29425713]\n[-0.95572629  0.29425713]\n[-0.95572629  0.29425713]\n[-0.95572629  0.29425713]\n[-0.95572629  0.29425713]\n[-0.95572629  0.29425713]\n[-0.95572629  0.29425713]\n[-0.95572629  0.29425713]\n[-0.95572629  0.29425713]\n[-0.95572629  0.29425713]\n[-0.95572629  0.29425713]\n[-0.95572629  0.29425713]\n[-0.95572629  0.29425713]\n[-0.95572629  0.29425713]\n[-0.95572629  0.29425713]\n[-0.95572629  0.29425713]\n[-0.95572629  0.29425713]\n[-0.95572629  0.29425713]\nGradient descent with exact step size takes 100 iterations to solve for the minimiser with an error of 0.0010000\n"
     ]
    }
   ],
   "source": [
    "tol = 1e-3\n",
    "max_iterations = 100\n",
    "x_init = np.array([3.0,0.0], dtype=np.float64)\n",
    "\n",
    "num_iter, x_gd = grad_descent_p2(x_init, tol, max_iterations)\n",
    "\n",
    "print(\"Gradient descent with exact step size takes %d iterations to solve for the minimiser with an error of %.7f\"%(num_iter, tol))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}